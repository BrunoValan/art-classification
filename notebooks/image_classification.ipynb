{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms as T\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms to be applied to the images\n",
    "\n",
    "# Reference: https://github.com/AIPI540/AIPI540-Deep-Learning-Applications/blob/main/2_computer_vision/CNNs/transfer_learning.ipynb\n",
    "\n",
    "transform_train = T.Compose([\n",
    "    T.RandomResizedCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(degrees=30),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)), # normalize the pixel values according to the mean and std of ImageNet\n",
    "])\n",
    "\n",
    "transform_val_test = T.Compose([\n",
    "    T.Resize(256), \n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=(0.485, 0.456, 0.406), \n",
    "                std=(0.229, 0.224, 0.225)), # normalize the pixel values according to the mean and std of ImageNet\n",
    "])\n",
    "\n",
    "# Apply the training transform to the training set and load the training set \n",
    "data_dir = '../data/final_output_data'\n",
    "dataset_train = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform_train)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# Apply the validation & test transform to the validation set and load the validation set \n",
    "dataset_val = datasets.ImageFolder(os.path.join(data_dir, 'val'), transform_val_test)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "# Apply the validation & test transform to the test set and load the test set \n",
    "dataset_test = datasets.ImageFolder(os.path.join(data_dir, 'test'), transform_val_test)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=True, num_workers=4)\n",
    "\n",
    "dataloaders = {'train': dataloader_train, \n",
    "               'val': dataloader_val, \n",
    "               'test': dataloader_test}\n",
    "dataset_sizes = {'train': len(dataset_train), \n",
    "                 'val': len(dataset_val), \n",
    "                 'test': len(dataset_test)}\n",
    "print(dataset_sizes)\n",
    "class_names = dataset_train.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dataloader_train))\n",
    "images = images.numpy()\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for idx in np.arange(BATCH_SIZE):\n",
    "    ax = fig.add_subplot(2, BATCH_SIZE//2, idx+1, xticks=[], yticks=[])\n",
    "    image = images[idx]\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    image = np.clip(image, 0, 1)\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(\"{}\".format(class_names[labels[idx]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate pre-trained resnet\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "# Shut off autograd for all layers to freeze model so the layer weights are not trained\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of inputs to final Linear layer\n",
    "num_features = model.fc.in_features\n",
    "\n",
    "# Replace final Linear layer with a new Linear with the same number of inputs but just 2 outputs (2 classes)\n",
    "model.fc = nn.Linear(num_features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the model (after modification)\n",
    "summary(model, (images.shape[1:]),batch_size=BATCH_SIZE, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss combines softmax and nn.NLLLoss() in one single class.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LR)\n",
    "\n",
    "# Learning rate scheduler - decay LR by a factor of 0.1 every 7 epochs\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for model training & evaluation\n",
    "def train_model(model, criterion, optimizer, dataloaders, scheduler, device, num_epochs, defrost):\n",
    "    model = model.to(device) # Send model to GPU if available\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Get the input images and labels, and send to GPU if available\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the weight gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass to get outputs and calculate loss\n",
    "                # Track gradient only for training data\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backpropagation to get the gradients with respect to each weight\n",
    "                    # Only if in train\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        # Update the weights\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Convert loss into a scalar and add it to running_loss\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # Track number of correct predictions\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # Step along learning rate scheduler when in train\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            # Calculate and display average loss and accuracy for the epoch\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # If model performs better on val set, save weights as the best model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:3f}'.format(best_acc))\n",
    "\n",
    "    # Load the weights from best model\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    # \"defrost\" all the model parameters by turning on autograd for all layers so the layer weights are also trained\n",
    "    if defrost: \n",
    "        print(\"unfreeze the model and start training again:\" )\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        since = time.time()\n",
    "        num_epochs_unfreeze = 5 # defrost parameter training with 5 epochs\n",
    "        for epoch in range(1, num_epochs_unfreeze+1): \n",
    "            print('Epoch {}/{}'.format(epoch, num_epochs_unfreeze))\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Get the input images and labels, and send to GPU if available\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    # Zero the weight gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass to get outputs and calculate loss\n",
    "                    # Track gradient only for training data\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # Backpropagation to get the gradients with respect to each weight\n",
    "                        # Only if in train\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            # Update the weights\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Convert loss into a scalar and add it to running_loss\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    # Track number of correct predictions\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                # Step along learning rate scheduler when in train\n",
    "                if phase == 'train':\n",
    "                    scheduler.step()\n",
    "\n",
    "                # Calculate and display average loss and accuracy for the epoch\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "                # If model performs better on val set, save weights as the best model\n",
    "                if phase == 'val' and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:3f}'.format(best_acc))\n",
    "\n",
    "        # Load the weights from best model\n",
    "        model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save the model\n",
    "model = train_model(model, criterion, optimizer, dataloaders, lr_scheduler, device, num_epochs=NUM_EPOCH, defrost=True)\n",
    "torch.save(model.state_dict(), \"../models/model4.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the existing model checkpoints\n",
    "model.load_state_dict(torch.load(\"models/model4.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device) # Send model to GPU if available\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    # Get all test images (every image in one batch)\n",
    "    images, labels = next(iter(dataloader_test))\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    # Get predictions\n",
    "    _,preds = torch.max(model(images), 1)\n",
    "    # convert predictions & labels to numpy array format\n",
    "    y_pred = np.squeeze(preds.cpu().numpy())\n",
    "    y_true = np.squeeze(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
